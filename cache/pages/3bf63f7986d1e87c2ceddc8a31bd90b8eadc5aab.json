{
  "url": "https://github.com/Jiaxin-Wen/CPM-Live/compare/8c74cda...OpenBMB:CPM-Live:8ad7aa6.diff",
  "title": "",
  "text": "diff --git a/.gitignore b/.gitignore index 9081237..06fed04 100644 --- a/.gitignore +++ b/.gitignore @@ -134,4 +134,9 @@ dmypy.json *.bin *.idx -*.pt \\ No newline at end of file +*.pt + +data +data_raw +results +pretrain_data \\ No newline at end of file diff --git a/README-ZH.md b/README-ZH.md new file mode 100644 index 0000000..3ce63b7 --- /dev/null +++ b/README-ZH.md @@ -0,0 +1,60 @@ + + +## 动态 +- 2023/05/27 [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) 发布了！ +- 2023/04/12 CPM-Ant 可以在[HuggingFace Transformers](https://huggingface.co/openbmb/cpm-ant-10b)中使用了！ +- 2022/10/12 中英双语模型 [CPM-Ant+](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live) 已经发布！除了能够生成中文/英文文本，现在模型还可以处理问答、摘要和翻译任务！ +- 2022/09/16 [CPM-Ant](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) 已经发布！ +- 2022/05/29 CPM-Live的训练今天启动! 详情请查看[训练动态](https://live.openbmb.org/home)。 +- 2022/05/25 CPM-Live的[训练计划](./plans/CPM-Live训练计划书.md)现已公布。期待训练开始! + + +## 里程碑 +- **CPM-Bee** (2022/10/13-2023/05/27) [[代码](https://github.com/OpenBMB/CPM-Bee)][[模型](https://github.com/OpenBMB/CPM-Bee#%E6%A8%A1%E5%9E%8B)][[计划书](./plans/CPM-Bee训练计划书.md)] +- **CPM-Ant+** (2022/08/05-2022/10/12) [[代码](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live)][[模型](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live#model-checkpoints)] +- **CPM-Ant** (2022/05/29-2022/08/05) [[代码](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)][[模型](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live#model-checkpoints)][[网站](https://live.openbmb.org/ant)][[博客](https://www.openbmb.org/en/community/blogs/blogpage?id=98afef2ce45f4fe9a4bc15a66d7ccb92)][[计划书](./plans/CPM-Ant训练计划书.md)] + +## 训练计划 +考虑到数据和计算资源的规模，CPM-Live将从10B模型开始训练并持续学习。 + +### 在训练过程中，我们将进行： + +- **实时**：显示模型训练指标 +- **每天**：发布模型训练日志 +- **每周**：处理社区的讨论和反馈 +- **不定期**：在模型训练期间发布允许公开下载的检查点 + + +### 在训练期间你可以： + +- **提出你的模型倡议**：对模型架构、训练方法或数据源有好的想法？你可以在社区里提出你的模型倡议。如果该倡议得到更多的支持并且实际可行，我们将把它添加到我们正在训练的模型中，这样CPM-Live就可以在大家的帮助下不断学习和进步。 + +- **开发你的应用程序**：基于CPM-Live，你可以向社区提交你初期想法、原型、开发代码或完成的应用程序。我们将在网站上展示最受欢迎的应用程序。 + +- **在论坛上聊天**：你可以在我们的论坛上谈论任何与大模型有关的话题，如学术研究、工程实现、工具使用、应用设计等。无论你是否有经验，我们相信每个人都可以从积极和开放的讨论中受益。 + +- **下载资源**：模型训练完成后，你可以在开放使用许可下自由下载模型参数。CPM-Live使用的是包括商业化许可的开放许可。通过模型压缩和推理加速工具，你可以在自己的电脑上体验大模型的威力! + + + +## 社区 + +我们的[社区](https://github.com/OpenBMB/CPM-Live/discussions) 基于GitHub Discussions。 + +阅读[第一篇帖子](https://github.com/OpenBMB/CPM-Live/discussions/1)，开始你对CPM-Live的探索吧！ + + + + + diff --git a/README.md b/README.md index c809a39..4c0d942 100644 --- a/README.md +++ b/README.md @@ -5,20 +5,29 @@ **Live Training for Open-source Big Models** - Website • Plan • Discussion + Website • Plan • Discussion • 简体中文 + - ## What's New +- 2023/05/27 [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) is released! +- 2023/04/12 CPM-Ant has been integrated into [HuggingFace Transformers](https://huggingface.co/openbmb/cpm-ant-10b)! +- 2022/10/12 [CPM-Ant+](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live), a bilingual model, is released! In addition to generating Chinese/English text, you can now use our model for QA, summarization and translation tasks! +- 2022/09/16 [CPM-Ant](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live) is released! - 2022/05/29 The training of CPM-Live has launched today! See [training dynamics](https://live.openbmb.org/home). - 2022/05/25 The [training plan](./plans/CPM-Live训练计划书.md) for CPM-Live is now published. Look forward to the training! +## Milestones + +- **CPM-Bee** (2022/10/13-2023/05/27) [[Code](https://github.com/OpenBMB/CPM-Bee)][[Model](https://github.com/OpenBMB/CPM-Bee#%E6%A8%A1%E5%9E%8B)][[Plan](./plans/CPM-Bee训练计划书.md)] +- **CPM-Ant+** (2022/08/05-2022/10/12) [[Code](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live)][[Model](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live#model-checkpoints)] +- **CPM-Ant** (2022/05/29-2022/08/05) [[Code](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)][[Model](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live#model-checkpoints)][[Website](https://live.openbmb.org/ant)][[Blog](https://www.openbmb.org/en/community/blogs/blogpage?id=98afef2ce45f4fe9a4bc15a66d7ccb92)][[Plan](./plans/CPM-Ant训练计划书.md)] ## Training Plan -Considering the scale of data and computing resources, CPM-Live will start with a 10B model training, which we named CPM-Ant. The training of CPM-Ant will start on May 29, 2022, and the entire training process is expected to last five months. +Considering the scale of data and computing resources, CPM-Live will start with a 10B model training. ### During training we will do: @@ -43,8 +52,3 @@ Considering the scale of data and computing resources, CPM-Live will start with [Our community](https://github.com/OpenBMB/CPM-Live/discussions) is based on GitHub Discussions. Read the [first post](https://github.com/OpenBMB/CPM-Live/discussions/1) and start your exploration on CPM-Live! - - - - - diff --git a/cpm-live/.flake8 b/cpm-live/.flake8 index b357ab3..e4d2c92 100644 --- a/cpm-live/.flake8 +++ b/cpm-live/.flake8 @@ -3,4 +3,5 @@ per-file-ignores = # imported but unused __init__.py: F401 max-line-length = 100 -extend-ignore = E712, E203 \\ No newline at end of file +extend-ignore = E712, E203 +exclude = examples/*.py \\ No newline at end of file diff --git a/cpm-live/config/cpm-bee-10b.json b/cpm-live/config/cpm-bee-10b.json new file mode 100644 index 0000000..c34b2e0 --- /dev/null +++ b/cpm-live/config/cpm-bee-10b.json @@ -0,0 +1,14 @@ +{ + \"vocab_size\": 86583, + \"dim_model\": 4096, + \"dim_ff\" : 10240, + \"num_layers\" : 48, + \"num_heads\": 32, + \"dim_head\" : 128, + \"dropout_p\" : 0.0, + \"position_bias_num_buckets\" : 256, + \"position_bias_num_segment_buckets\": 256, + \"position_bias_max_distance\" : 2048, + \"eps\" : 1e-6, + \"half\" : true +} diff --git a/cpm-live/config/cpm-bee-3b.json b/cpm-live/config/cpm-bee-3b.json new file mode 100644 index 0000000..55fd0f2 --- /dev/null +++ b/cpm-live/config/cpm-bee-3b.json @@ -0,0 +1,14 @@ +{ + \"vocab_size\": 86580, + \"dim_model\": 2560, + \"dim_ff\" : 3072, + \"num_layers\" : 32, + \"num_heads\": 32, + \"dim_head\" : 80, + \"dropout_p\" : 0.0, + \"position_bias_num_buckets\" : 256, + \"position_bias_num_segment_buckets\": 256, + \"position_bias_max_distance\" : 2048, + \"eps\" : 1e-6, + \"half\" : true +} diff --git a/cpm-live/cpm_live/__init__.py b/cpm-live/cpm_live/__init__.py index 03e328b..e69de29 100644 --- a/cpm-live/cpm_live/__init__.py +++ b/cpm-live/cpm_live/__init__.py @@ -1,5 +0,0 @@ -from . import models -from . import dataset -from . import utils -from . import tokenizers -from .arguments import get_args diff --git a/cpm-live/cpm_live/arguments.py b/cpm-live/cpm_live/arguments.py index 2f3202c..b1b6d0e 100644 --- a/cpm-live/cpm_live/arguments.py +++ b/cpm-live/cpm_live/arguments.py @@ -29,13 +29,7 @@ def add_training_args(parser: argparse.ArgumentParser): group = parser.add_argument_group(\"train\", \"training configurations\") - group.add_argument( - \"--base-path\", - type=str, - default=None, - help=\"Path to the project base directory.\", - ) - group.add_argument(\"--dataset_name\", type=str, default=None, help=\"Name of the dataset\") + group.add_argument(\"--dataset\", type=str, default=\"dataset.json\", help=\"Path to dataset\") group.add_argument( \"--load\", type=str, @@ -54,18 +48,14 @@ def add_training_args(parser: argparse.ArgumentParser): default=None, help=\"Output filename to save checkpoints to.\", ) + group.add_argument( - \"--save-iters\", - type=int, - default=1000, - help=\"number of iterations between saves\", - ) - group.add_argument( - \"--log-dir\", + \"--tensorboard\", type=str, - default=\"logs\", - help=\"tensorboard log directory\", + default=None, + help=\"tensorboard directory\", ) + group.add_argument(\"--inspect-iters\", type=int, default=1000, help=\"number of inspecting\") group.add_argument(\"--batch-size\", type=int, default=32, help=\"Data Loader batch size\") group.add_argument(\"--clip-grad\", type=float, default=1.0, help=\"gradient clipping\") @@ -76,33 +66,12 @@ def add_training_args(parser: argparse.ArgumentParser): help=\"total number of iterations to train over all training runs\", ) group.add_argument(\"--max-length\", type=int, default=512, help=\"max length of input\") - group.add_argument( - \"--max-encoder-length\", - type=int, - default=512, - help=\"max length of encoder input\", - ) - group.add_argument( - \"--max-decoder-length\", - type=int, - default=256, - help=\"max length of decoder input\", - ) - group.add_argument( - \"--start-step\", type=int, default=0, help=\"step to start or continue training\" - ) - group.add_argument(\"--seed\", type=int, default=1234, help=\"random seed for reproducibility\") - group.add_argument( - \"--epochs\", - type=int, - default=1, - help=\"total number of epochs to train over all training runs\", - ) + group.add_argument(\"--seed\", type=int, default=1234, help=\"random seed for reproducibility\") # Learning rate. group.add_argument(\"--lr\", type=float, default=1.0e-4, help=\"initial learning rate\") - group.add_argument(\"--weight-decay\", type=float, default=1.0e-2, help=\"weight-decay\") + group.add_argument(\"--weight-decay\", type=float, default=1.0e-2, help=\"weight decay rate\") group.add_argument(\"--loss-scale\", type=float, default=65536, help=\"loss scale\") group.add_argument( @@ -111,13 +80,6 @@ def add_training_args(parser: argparse.ArgumentParser): default=0.01, help=\"percentage of data to warmup on (.01 = 1% of all \" \"training iters). Default 0.01\", ) - group.add_argument( - \"--lr-decay-iters\", - type=int, - default=None, - help=\"number of iterations to decay LR over,\" - \" If None defaults to `--train-iters`*`--epochs`\", - ) group.add_argument( \"--lr-decay-style\", type=str, @@ -125,20 +87,62 @@ def add_training_args(parser: argparse.ArgumentParser): choices=[\"constant\", \"linear\", \"cosine\", \"exponential\", \"noam\"], help=\"learning rate decay function\", ) + group.add_argument(\"--lr-decay-iters\", type=int, default=None, help=\"lr decay steps\") group.add_argument( - \"--local_rank\", + \"--start-step\", type=int, default=0, help=\"step to start or continue training\" + ) + + return parser + + +def add_pretrain_args(parser: argparse.ArgumentParser): + group = parser.add_argument_group(\"pretrain\", \"pretrain configurations\") + group.add_argument( + \"--save-iters\", type=int, + default=1000, + help=\"number of iterations between saves\", + ) + group.add_argument( + \"--log-dir\", + type=str, default=None, - help=\"local rank passed from distributed launcher\", + help=\"log directory\", ) return parser -def get_args(): +def add_finetune_args(parser: argparse.ArgumentParser): + group = parser.add_argument_group(\"finetune\", \"fintune configurations\") + group.add_argument(\"--epoch\", type=int, default=1, help=\"number of training epochs\") + group.add_argument(\"--task-name\", type=str, default=\"task\", help=\"name of training task\") + group.add_argument( + \"--use-delta\", + action=\"store_true\", + default=False, + help=\"use delta tuning or not\" + ) + group.add_argument(\"--eval_dataset\", type=str, help=\"path to eval dataset\") + group.add_argument( + \"--drop-last\", + action=\"store_true\", + default=False, + help=\"drop data from each epoch that cannot be formed into a complete batch at the end\", + ) + group.add_argument(\"--eval-interval\", type=int, default=500, help=\"eval interval\") + group.add_argument(\"--early-stop-patience\", type=int, default=5, help=\"early stop steps\") + return parser + + +def get_args(pretrain: bool = False, finetune: bool = False): parser = argparse.ArgumentParser() parser = add_model_config_args(parser) parser = add_training_args(parser) + if pretrain: + parser = add_pretrain_args(parser) + if finetune: + parser = add_finetune_args(parser) args = parser.parse_args() return args diff --git a/cpm-live/cpm_live/dataset/distributed_dataset.py b/cpm-live/cpm_live/dataset/distributed_dataset.py index d90c612..a9979da 100644 --- a/cpm-live/cpm_live/dataset/distributed_dataset.py +++ b/cpm-live/cpm_live/dataset/distributed_dataset.py @@ -15,29 +15,35 @@ import io import os -import pickle -from typing import List +import struct +from typing import List, Optional, Set import torch import bisect import bmtrain as bmt - +import json +from .serializer import Serializer, PickleSerializer import random import string +import time def _random_string(): return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=8)) +_DEFAULT_BLOCK_SIZE = 16 << 20 + + class FileInfo: def __init__( self, - file_name: str, - block_begin: int, - block_end: int, - nbytes: int, - nlines: int, + file_name: str = \"\", + block_begin: int = 0, + block_end: int = 0, + nbytes: int = 0, + nlines: int = 0, mask: bool = False, + block_size: int = _DEFAULT_BLOCK_SIZE, ) -> None: self.file_name = file_name self.block_begin = block_begin @@ -45,34 +51,153 @@ def __init__( self.nbytes = nbytes self.nlines = nlines self.mask = mask + self.block_size = block_size - @classmethod - def _load_from_state(cls, version, data): - if version == 1: - file_name, block_begin, block_end, nbytes, nlines, mask = data - return cls(file_name, block_begin, block_end, nbytes, nlines, mask) - else: - raise RuntimeError(\"Unsupported version %d\" % version) - - def __reduce__(self): - return ( - FileInfo._load_from_state, - ( - 1, - ( - self.file_name, - self.block_begin, - self.block_end, - self.nbytes, - self.nlines, - self.mask, - ), - ), - ) + def state_dict(self): + return { + \"file_name\": self.file_name, + \"block_begin\": self.block_begin, + \"block_end\": self.block_end, + \"nbytes\": self.nbytes, + \"nlines\": self.nlines, + \"mask\": self.mask, + \"block_size\": self.block_size, + } + + def load_state_dict(self, d): + self.file_name = d[\"file_name\"] + self.block_begin = d[\"block_begin\"] + self.block_end = d[\"block_end\"] + self.nbytes = d[\"nbytes\"] + self.nlines = d[\"nlines\"] + self.mask = d[\"mask\"] + self.block_size = d[\"block_size\"] + + def dumps(self) -> str: + return json.dumps(self.state_dict()) + + def loads(self, data: str) -> \"FileInfo\": + self.load_state_dict(json.loads(data)) + return self + + def dump(self, fp: io.TextIOWrapper) -> \"FileInfo\": + fp.write(self.dumps()) + return self + + def load(self, fp: io.TextIOWrapper) -> \"FileInfo\": + self.loads(fp.read()) + return self + + +def _read_info_list(meta_path: str) -> List[FileInfo]: + info: List[FileInfo] = [] + while True: + try: + with open(meta_path, \"r\", encoding=\"utf-8\") as f: + for line in f.readlines(): + line = line.strip() + if len(line) > 0: + info.append(FileInfo().loads(line)) + return info + except Exception as e: + print(\"Error: reading info list in _read_info_list!,meta_path={path}, err={err}\". + format(path=meta_path, err=str(e))) + time.sleep(10) + + +def _write_info_list(meta_path: str, info: List[FileInfo]): + base_path = os.path.dirname(meta_path) + random_fname = os.path.join(base_path, \".meta.bin.%s\" % _random_string()) + while True: + try: + with open(random_fname, \"w\", encoding=\"utf-8\") as f: + for v in info: + f.write(v.dumps() + \"\\n\") + os.rename(random_fname, meta_path) + return + except Exception: + print(\"Error: writing info list!\") + time.sleep(10) -_MASK_VALUE = 0x7FFFFFFF -_DEFAULT_BLOCK_SIZE = 16 << 20 +def _filtered_range( + begin: int, end: int, rank: int, world_size: int, filter_set: Optional[Set[int]] = None +): + begin = begin + (rank + (world_size - (begin % world_size))) % world_size + + if filter_set is not None: + return [i for i in range(begin, end, world_size) if i in filter_set] + else: + return [i for i in range(begin, end, world_size)] + + +# for some bugs that may exist in hdfs +class SafeFile: + + def __init__(self, fname, mode): + self.fname = None + self.mode = None + self._fp = None + self.open_file(fname, mode) + + def read(self, size=-1): + if self._fp is None: + raise RuntimeError(\"Dataset is closed\") + try: + res = self._fp.read(size) + self.offset = self._fp.tell() + return res + except Exception as e: + print(\"Error {}: reading blocks in read {}!\".format(e, self.fname)) + self.open_file(self.fname, self.mode, self.offset) + return self.read(size) + + def tell(self): + if self._fp is None: + raise RuntimeError(\"Dataset is closed\") + try: + res = self._fp.tell() + self.offset = res + return res + except Exception as e: + print(\"Error {}: reading blocks in tell {}!\".format(e, self.fname)) + self.open_file(self.fname, self.mode, self.offset) + return self.tell() + + def seek(self, offset, whence=0): + if self._fp is None: + raise RuntimeError(\"Dataset is closed\") + try: + res = self._fp.seek(offset, whence) + self.offset = self._fp.tell() + return res + except Exception as e: + print(\"Error {}: reading blocks in seek {}!\".format(e, self.fname)) + self.open_file(self.fname, self.mode, self.offset) + return self.seek(offset, whence) + + def close(self): + if self._fp is not None: + try: + self._fp.close() + except Exception: + pass + self._fp = None + + def open_file(self, fname, mode, offset=None): + if not os.path.exists(fname): + raise RuntimeError(\"Dataset does not exist\") + try: + self.fname = fname + self.mode = mode + self._fp = open(fname, mode) + if offset is not None: + self._fp.seek(offset, io.SEEK_SET) + self.offset = self._fp.tell() + except Exception as e: + print(\"Error {}: reading blocks in open_file {}!\".format(e, self.fname)) + time.sleep(10) + self.open_file(fname, mode, offset) class DistributedDataset: @@ -100,16 +225,24 @@ def __init__( path: str, rank: int = 0, world_size: int = 1, - block_size=_DEFAULT_BLOCK_SIZE, + serializer: Optional[Serializer] = None, + max_repeat_times: Optional[int] = None, + shuffle: bool = True, ) -> None: # config self._path = path self._rank = rank self._world_size = world_size - self._block_size = block_size + self._max_repeat_times = max_repeat_times + self._repeat_times = 0 + self._shuffle = shuffle + + if serializer is None: + serializer = PickleSerializer() + self.serializer = serializer # dataset meta - self._block_states = torch.tensor([], dtype=torch.int) + self._unused_block: List[int] = [] self._file_info: List[FileInfo] = [] self._file_ends: List[int] = [] self._total_blocks = 0 @@ -124,12 +257,21 @@ def __init__( self._last_mod_time = 0 self._curr_fname = None - self._update_states() + self._update_states(fast_skip=False) + self._repeat_times += 1 def _update_states(self, fast_skip: bool = True): meta_path = os.path.join(self._path, \"meta.bin\") - mod_time = os.stat(meta_path).st_mtime + while True: + try: + mod_time = os.stat(meta_path).st_mtime + break + except Exception as e: + print(\"Error: reading info list in DistributedDataset._update_states, \" + \"meta_path={path}, err={err}!\".format(path=meta_path, err=str(e))) + time.sleep(10) + if self._last_mod_time < mod_time: # file changed pass @@ -139,12 +281,13 @@ def _update_states(self, fast_skip: bool = True): info: List[FileInfo] = [] if os.path.exists(meta_path): - with open(meta_path, \"rb\") as f: - info = pickle.load(f) + info = _read_info_list(meta_path) old_len = len(self._file_info) if old_len > len(info): raise RuntimeError(\"Dataset meta file: changed unexpectly\") + + mask_changed = False for i in range(old_len): if self._file_info[i].file_name != info[i].file_name: raise RuntimeError(\"Dataset meta file: changed unexpectly\") @@ -152,6 +295,8 @@ def _update_states(self, fast_skip: bool = True): raise RuntimeError(\"Dataset meta file: changed unexpectly\") if self._file_info[i].block_end != info[i].block_end: raise RuntimeError(\"Dataset meta file: changed unexpectly\") + if self._file_info[i].mask != info[i].mask: + mask_changed = True if info[0].block_begin != 0: raise RuntimeError(\"Dataset meta file: block error (0)\") @@ -159,7 +304,7 @@ def _update_states(self, fast_skip: bool = True): if info[i].block_end != info[i + 1].block_begin: raise RuntimeError(\"Dataset meta file: block error (%d)\" % (i + 1)) - if old_len == len(info) and fast_skip: + if (old_len == len(info) and not mask_changed) and fast_skip: # fast skip return @@ -176,32 +321,38 @@ def _update_states(self, fast_skip: bool = True): self._nlines = 0 if total_blocks > 0: - masks = torch.full( - (total_blocks,), - _MASK_VALUE, - dtype=torch.int, - device=\"cpu\", - requires_grad=False, - ) - masks[self._rank :: self._world_size] = 0 - for v in info: - if v.mask or (not os.path.exists(self._get_file_path(v.file_name))): - masks[v.block_begin : v.block_end] = _MASK_VALUE - new_block_states = torch.zeros( - total_blocks, dtype=torch.int, device=\"cpu\", requires_grad=False - ) - new_block_states[: self._block_states.size(0)] = self._block_states - new_block_states = torch.maximum(new_block_states, masks) - - self._block_states = new_block_states + unused_block_set = set(self._unused_block) + nw_unused_block: List[int] = [] + for i in range(len(info)): + v = info[i] + if not v.mask: + if i < old_len: + nw_unused_block.extend( + _filtered_range( + v.block_begin, + v.block_end, + self._rank, + self._world_size, + unused_block_set, + ) + ) + else: + nw_unused_block.extend( + _filtered_range( + v.block_begin, v.block_end, self._rank, self._world_size + ) + ) + + # re-shuffle unused blocks + if self._shuffle: + random.shuffle(nw_unused_block) + self._unused_block = nw_unused_block self._file_ends = [] for v in info: self._file_ends.append(v.block_end) else: - self._block_states = torch.tensor( - [], dtype=torch.int, device=\"cpu\", requires_grad=False - ) + self._unused_block = [] self._file_ends = [] self._total_blocks = total_blocks self._file_info = info @@ -209,27 +360,62 @@ def _update_states(self, fast_skip: bool = True): assert len(self._file_ends) == len(self._file_info) def _mask_file(self, f: FileInfo): - masks = torch.full( - (self._total_blocks,), 0, dtype=torch.int, device=\"cpu\", requires_grad=False - ) - masks[f.block_begin : f.block_end] = _MASK_VALUE - self._block_states = torch.maximum(self._block_states, masks) + self._unused_block = [ + block_id + for block_id in self._unused_block + if block_id < f.block_begin or block_id >= f.block_end + ] def _get_block_file(self, block_id: int): # find block in which file file_idx = bisect.bisect_right(self._file_ends, block_id) return self._file_info[file_idx] + def _prepare_new_epoch(self): + if self._max_repeat_times is not None: + if self._repeat_times >= self._max_repeat_times: + raise EOFError(\"End of dataset\") + nw_unused_block: List[int] = [] + for v in self._file_info: + if not v.mask: + nw_unused_block.extend( + _filtered_range(v.block_begin, v.block_end, self._rank, self._world_size) + ) + if self._shuffle: + random.shuffle(nw_unused_block) + self._unused_block = nw_unused_block + self._repeat_times += 1 + def _get_next_block(self): self._update_states() - if self._block_states.size(0) == 0: - raise RuntimeError(\"Empty dataset\") - mn_block: int = self._block_states.argmin().item() # type: ignore - if self._block_states[mn_block].item() == _MASK_VALUE: - raise RuntimeError(\"Empty dataset\") - self._block_states[mn_block] += 1 + if len(self._unused_block) == 0: + self._prepare_new_epoch() + if len(self._unused_block) == 0: + raise RuntimeError(\"Empty dataset {}\".format(self._path)) + + mn_block: int = self._unused_block.pop() return mn_block + def _state_dict(self): + self._update_states() + num_unused_block = len(self._unused_block) + if (self._fp is not None) and (self._curr_block is not None): + curr_block = self._curr_block + curr_f = self._get_block_file(curr_block) + inblock_offset = self._fp.tell() - (curr_block - curr_f.block_begin) * curr_f.block_size + else: + curr_block = -1 + inblock_offset = 0 + + return { + \"states\": torch.tensor(self._unused_block, dtype=torch.long, device=\"cpu\"), + \"block\": torch.tensor( + [curr_block, inblock_offset, num_unused_block, self._repeat_times], + dtype=torch.long, + device=\"cpu\", + ), + } + def state_dict(self): \"\"\"Returns a state dict representing the read states of the dataset. @@ -238,32 +424,43 @@ def state_dict(self): >>> dataset.load_state_dict(state) \"\"\" self._update_states() - states = torch.where( - self._block_states == _MASK_VALUE, - torch.zeros(self._total_blocks, dtype=torch.int, device=\"cpu\", requires_grad=False), - self._block_states, - ) + num_unused_block = len(self._unused_block) if (self._fp is not None) and (self._curr_block is not None): curr_block = self._curr_block curr_f = self._get_block_file(curr_block) - inblock_offset = self._fp.tell() - (curr_block - curr_f.block_begin) * self._block_size + inblock_offset = self._fp.tell() - (curr_block - curr_f.block_begin) * curr_f.block_size else: curr_block = -1 inblock_offset = 0 with torch.no_grad(): if self._world_size > 1: - gpu_states = states.cuda() - gpu_block = torch.tensor([curr_block, inblock_offset], dtype=torch.long).cuda() - global_states = bmt.distributed.all_reduce(gpu_states, op=\"sum\").cpu() - global_block = bmt.distributed.all_gather(gpu_block).cpu() + gpu_num_unused_block = torch.tensor([num_unused_block], dtype=torch.long).cuda() + max_unused_blocks = ( + bmt.distributed.all_reduce(gpu_num_unused_block, op=\"max\").cpu().item() + ) + gpu_states = torch.full((max_unused_blocks,), -1, dtype=torch.long).cuda() + gpu_states[:num_unused_block] = torch.tensor( + self._unused_block, dtype=torch.long + ).cuda() + + gpu_block = torch.tensor( + [curr_block, inblock_offset, num_unused_block, self._repeat_times], + dtype=torch.long, + ).cuda() + global_states = bmt.distributed.all_gather( + gpu_states + ).cpu() # (world_size, max_unused_blocks) + global_block = bmt.distributed.all_gather(gpu_block).cpu() # (world_size, 4) return {\"states\": global_states, \"block\": global_block} else: return { - \"states\": states, + \"states\": torch.tensor([self._unused_block], dtype=torch.long, device=\"cpu\"), \"block\": torch.tensor( - [[curr_block, inblock_offset]], dtype=torch.long, device=\"cpu\" + [[curr_block, inblock_offset, num_unused_block, self._repeat_times]], + dtype=torch.long, + device=\"cpu\", ), } @@ -278,11 +475,10 @@ def load_state_dict(self, state, strict: bool = True): >>> state = dataset.state_dict() >>> \"\"\" + block_states: torch.LongTensor = state[\"states\"] + block_info: torch.LongTensor = state[\"block\"] - self._block_states = state[\"states\"] - self._update_states(False) - - if state[\"block\"].size(0) != self._world_size: + if block_states.size(0) != self._world_size: if strict: raise ValueError( \"world_size changed (%d -> %d)\" % (state[\"block\"].size(0), self._world_size) @@ -291,20 +487,48 @@ def load_state_dict(self, state, strict: bool = True): self._curr_block = None self._fp = None self._curr_fname = None + self._repeat_times = int(block_info[0, 3].item()) + + # re-shuffle unused blocks + nw_unused_block: List[int] = [] + for i in range(block_states.size(0)): + # filter blocks that are not in this rank + num_unused_blocks: int = int(block_info[i, 2].item()) + nw_unused_block.extend( + [ + block_id + for block_id in block_states[i, :num_unused_blocks].tolist() + if block_id % self._world_size == self._rank + ] + ) + if self._shuffle: + random.shuffle(nw_unused_block) + self._unused_block = nw_unused_block else: - curr_block = state[\"block\"][self._rank][0].item() - inblock_offset = state[\"block\"][self._rank][1].item() + curr_block, inblock_offset, num_unused_blocks, self._repeat_times = block_info[ + self._rank + ].tolist() if curr_block == -1: self._curr_block = None else: - self._curr_block = curr_block - f_info = self._get_block_file(self._curr_block) - self._open_file( - f_info.file_name, - (self._curr_block - f_info.block_begin) * self._block_size + inblock_offset, - ) + while True: + try: + self._curr_block = curr_block + f_info = self._get_block_file(self._curr_block) + self._open_file( + f_info.file_name, + (self._curr_block - f_info.block_begin) + * f_info.block_size + + inblock_offset, + ) + self._unused_block = block_states[self._rank, :num_unused_blocks].tolist() + break + except Exception: + print(\"Error: reading block!\") + time.sleep(10) # end + self._update_states() def _get_file_path(self, fname): return os.path.join(self._path, fname) @@ -314,11 +538,11 @@ def _open_file(self, fname, offset): if self._fp is not None: self._fp.close() self._curr_fname = None - self._fp = open(self._get_file_path(fname), \"rb\") + # self._fp = open(self._get_file_path(fname), \"rb\") + self._fp = SafeFile(self._get_file_path(fname), \"rb\") self._curr_fname = fname else: assert self._fp is not None, \"Unexpected error\" - self._fp.seek(offset, io.SEEK_SET) # move to block def read(self): @@ -332,10 +556,11 @@ def read(self): try: self._open_file( f_info.file_name, - (next_block_id - f_info.block_begin) * self._block_size, + (next_block_id - f_info.block_begin) * f_info.block_size, ) self._curr_block = next_block_id except FileNotFoundError: + print(\"ERR: reading again!\") self._mask_file(f_info) return self.read() # read again @@ -345,7 +570,9 @@ def read(self): MAGIC = self._fp.read(1) if MAGIC == b\"\\x1F\": # correct - return pickle.load(self._fp) + size = struct.unpack(\"I\", self._fp.read(4))[0] + data = self._fp.read(size) + return self.serializer.deserialize(data) elif MAGIC == b\"\\x00\": # end of block self._curr_block = None @@ -359,24 +586,27 @@ def nbytes(self): class SimpleDataset(DistributedDataset): - def __init__(self, path: str, block_size=_DEFAULT_BLOCK_SIZE) -> None: - super().__init__(path, 0, 1, block_size) - - def _get_next_block(self): - self._update_states() - if self._block_states.size(0) == 0: - raise RuntimeError(\"Empty dataset\") - mn_block: int = self._block_states.argmin().item() # type: ignore - if self._block_states[mn_block].item() >= 1: - raise EOFError(\"no more data\") - self._block_states[mn_block] += 1 - return mn_block + def __init__( + self, + path: str, + serializer: Optional[Serializer] = None, + shuffle: bool = True, + ) -> None: + super().__init__( + path, + 0, + 1, + serializer=serializer, + max_repeat_times=1, + shuffle=shuffle, + ) def __iter__(self): while True: try: data = self.read() except EOFError: + self._repeat_times = 0 break yield data @@ -385,7 +615,7 @@ def __len__(self): class DatasetWriter: - def __init__(self, fname, block_size): + def __init__(self, fname: str, block_size: int, serializer: Optional[Serializer] = None): self._fname = fname self._block_size = block_size self._fp = open(self._fname, \"wb\") @@ -395,6 +625,10 @@ def __init__(self, fname, block_size): self._nlines = 0 self._nblocks = 1 + if serializer is None: + serializer = PickleSerializer() + self.serializer = serializer + def write(self, data): \"\"\"Write a piece of data into dataset. @@ -405,7 +639,8 @@ def write(self, data): >>> writer.write( \"anything you want\" ) \"\"\" - byte_data = pickle.dumps(data) + byte_data = self.serializer.serialize(data) + byte_data = struct.pack(\"I\", len(byte_data)) + byte_data if self._inblock_offset + 2 + len(byte_data) > self._block_size: self._fp.write( b\"\\x00\" * (self._block_size - self._inblock_offset) @@ -442,10 +677,19 @@ def close(self): class DatasetBuilder: - def __init__(self, path: str, dbname: str, block_size=_DEFAULT_BLOCK_SIZE) -> None: + def __init__( + self, + path: str, + dbname: str, + block_size=_DEFAULT_BLOCK_SIZE, + serializer: Optional[Serializer] = None, + ) -> None: self._block_size = block_size self._path = path self._dbname = dbname + if serializer is None: + serializer = PickleSerializer() + self.serializer = serializer if not os.path.exists(self._path): os.makedirs(self._path) @@ -454,8 +698,7 @@ def __init__(self, path: str, dbname: str, block_size=_DEFAULT_BLOCK_SIZE) -> No info: List[FileInfo] = [] if os.path.exists(meta_path): - with open(meta_path, \"rb\") as f: - info = pickle.load(f) + info = _read_info_list(meta_path) for v in info: if v.file_name == dbname: @@ -466,7 +709,7 @@ def __init__(self, path: str, dbname: str, block_size=_DEFAULT_BLOCK_SIZE) -> No raise ValueError(\"File exists `%s`\" % self._db_path) def __enter__(self): - self._writer = DatasetWriter(self._db_path, self._block_size) + self._writer = DatasetWriter(self._db_path, self._block_size, self.serializer) return self._writer def __exit__(self, exc_type, exc_value, exc_traceback): @@ -482,8 +725,7 @@ def __exit__(self, exc_type, exc_value, exc_traceback): meta_path = os.path.join(self._path, \"meta.bin\") info: List[FileInfo] = [] if os.path.exists(meta_path): - with open(meta_path, \"rb\") as f: - info = pickle.load(f) + info = _read_info_list(meta_path) last_block = 0 if len(info) > 0: last_block = info[-1].block_end @@ -495,19 +737,22 @@ def __exit__(self, exc_type, exc_value, exc_traceback): self._writer.nbytes, self._writer.nlines, False, + self._block_size, ) ) # atomic write to meta file - random_fname = os.path.join(self._path, \".meta.bin.%s\" % _random_string()) - with open(random_fname, \"wb\") as f: - pickle.dump(info, f) - os.rename(random_fname, meta_path) + _write_info_list(meta_path, info) self._writer = None -def build_dataset(path: str, dbname: str, block_size: int = _DEFAULT_BLOCK_SIZE): +def build_dataset( + path: str, + dbname: str, + block_size: int = _DEFAULT_BLOCK_SIZE, + serializer: Optional[Serializer] = None, +): \"\"\"Open the dataset in write mode and returns a writer. Args: @@ -520,4 +765,4 @@ def build_dataset(path: str, dbname: str, block_size: int = _DEFAULT_BLOCK_SIZE) >>> for i in range(10): >>> writer.write( { \"anything you want\" } ) \"\"\" # noqa: E501 - return DatasetBuilder(path, dbname, block_size) + return DatasetBuilder(path, dbname, block_size=block_size, serializer=serializer) diff --git a/cpm-live/cpm_live/dataset/serializer.py b/cpm-live/cpm_live/dataset/serializer.py new file mode 100644 index 0000000..ef3865f --- /dev/null +++ b/cpm-live/cpm_live/dataset/serializer.py @@ -0,0 +1,61 @@ +# coding=utf-8 +# Copyright 2020 The OpenBMB team. All rights reserved. +# +# Licensed under the Apache License, Version 2.0 (the \"License\"); +# you may not use this file except in compliance with the License. +# You may obtain a copy of the License at +# +# http://www.apache.org/licenses/LICENSE-2.0 +# +# Unless required by applicable law or agreed to in writing, software +# distributed under the License is distributed on an \"AS IS\" BASIS, +# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. +# See the License for the specific language governing permissions and +# limitations under the License. + +import pickle +import json + + +class Serializer: + def __init__(self) -> None: + pass + + def serialize(self, obj) -> bytes: + raise NotImplementedError() + + def deserialize(self, data: bytes): + raise NotImplementedError() + + +class PickleSerializer(Serializer): + def __init__(self) -> None: + pass + + def serialize(self, obj) -> bytes: + return pickle.dumps(obj) + + def deserialize(self, data: bytes): + return pickle.loads(data) + + +class JsonSerializer(Serializer): + def __init__(self) -> None: + pass + + def serialize(self, obj) -> bytes: + return json.dumps(obj, ensure_ascii=False).encode(\"utf-8\") + + def deserialize(self, data: bytes): + return json.loads(data.decode(\"utf-8\")) + + +class RawSerializer(Serializer): + def __init__(self) -> None: + pass + + def serialize(self, obj) -> bytes: + return obj + + def deserialize(self, data: bytes): + return data diff --git a/cpm-live/cpm_live/dataset/utils.py b/cpm-live/cpm_live/dataset/utils.py index c166a56..2a6ace6 100644 --- a/cpm-live/cpm_live/dataset/utils.py +++ b/cpm-live/cpm_live/dataset/utils.py @@ -14,16 +14,19 @@ # limitations under the License. import os -from typing import List +import struct +from typing import List, Optional from .distributed_dataset import ( SimpleDataset, build_dataset, + _read_info_list, + _write_info_list, _random_string, _DEFAULT_BLOCK_SIZE, FileInfo, ) +from .serializer import RawSerializer import random -import pickle import shutil try: @@ -42,7 +45,8 @@ def shuffle_dataset( path_tgt: str, block_size: int = _DEFAULT_BLOCK_SIZE, bucket_size: int = _DEFAULT_SHUFFLE_BUCKET_SIZE, - progress_bar=False, + progress_bar: bool = False, + output_name: Optional[str] = None, ): \"\"\"Shuffle one distributed datataset, write results to another dataset. @@ -60,7 +64,7 @@ def shuffle_dataset( if progress_bar and not support_tqdm: raise RuntimeError(\"Requires `tqdm` to enable progress bar.\") - ds = SimpleDataset(path_src, block_size=block_size) + ds = SimpleDataset(path_src, serializer=RawSerializer()) num_buckets = (ds.nbytes + bucket_size - 1) // bucket_size tmp_files = [os.path.join(path_src, \".tmp.%s\" % _random_string()) for _ in range(num_buckets)] @@ -74,7 +78,8 @@ def shuffle_dataset( iterator = tqdm(ds, desc=\"Shuffle step 1/2\") for data in iterator: bucket_id = int(random.random() * num_buckets) - pickle.dump(data, f_tmp[bucket_id]) # write into a random bucket + len_data = len(data) + f_tmp[bucket_id].write(struct.pack(\"I\", len_data) + data) finally: # close all files for fp in f_tmp: @@ -83,7 +88,14 @@ def shuffle_dataset( f_tmp = [] # Step 2: shuffle inside bucket - with build_dataset(path_tgt, \"%s.shuffle\" % _random_string()) as writer: + if output_name is None: + output_name = \"%s.shuffle\" % _random_string() + with build_dataset( + path_tgt, + output_name, + block_size=block_size, + serializer=RawSerializer(), + ) as writer: iterator = tmp_files if progress_bar: iterator = tqdm(tmp_files, desc=\"Shuffle step 2/2\") @@ -93,7 +105,12 @@ def shuffle_dataset( data_in_bucket = [] while True: try: - data_in_bucket.append(pickle.load(fp)) + raw_data = fp.read(4) + if len(raw_data) == 0: + # EOF + break + len_data = struct.unpack(\"I\", raw_data)[0] + data_in_bucket.append(fp.read(len_data)) except EOFError: break random.shuffle(data_in_bucket) @@ -125,12 +142,11 @@ def compact_dataset(path: str): info: List[FileInfo] = [] if os.path.exists(meta_path): - with open(meta_path, \"rb\") as f: - info = pickle.load(f) + info = _read_info_list(meta_path) else: raise ValueError(\"Dataset not exists\") - nw_info = [] + nw_info: List[FileInfo] = [] curr_block = 0 for v in info: if not os.path.exists(v.file_name): @@ -146,14 +162,12 @@ def compact_dataset(path: str): v.nbytes, v.nlines, v.mask, + v.block_size, ) ) curr_block += num_file_block - random_fname = os.path.join(path, \".meta.bin.%s\" % _random_string()) - with open(random_fname, \"wb\") as f: - pickle.dump(nw_info, f) - os.rename(random_fname, meta_path) + _write_info_list(meta_path, nw_info) def mask_dataset(path: str, dbname: str, mask: bool = True): @@ -173,19 +187,14 @@ def mask_dataset(path: str, dbname: str, mask: bool = True): info: List[FileInfo] = [] if os.path.exists(meta_path): - with open(meta_path, \"rb\") as f: - info = pickle.load(f) + info = _read_info_list(meta_path) else: raise ValueError(\"Dataset not exists\") for v in info: if v.file_name == dbname: v.mask = mask - - random_fname = os.path.join(path, \".meta.bin.%s\" % _random_string()) - with open(random_fname, \"wb\") as f: - pickle.dump(info, f) - os.rename(random_fname, meta_path) + _write_info_list(meta_path, info) def merge_dataset(dst: str, src: str): @@ -195,15 +204,13 @@ def merge_dataset(dst: str, src: str): info_src: List[FileInfo] = [] if os.path.exists(meta_path_src): - with open(meta_path_src, \"rb\") as f: - info_src = pickle.load(f) + info_src = _read_info_list(meta_path_src) else: raise ValueError(\"Dataset not exists\") info_dst: List[FileInfo] = [] if os.path.exists(meta_path_dst): - with open(meta_path_dst, \"rb\") as f: - info_dst = pickle.load(f) + info_dst = _read_info_list(meta_path_dst) else: raise ValueError(\"Dataset not exists\") @@ -219,6 +226,7 @@ def merge_dataset(dst: str, src: str): v.nbytes, v.nlines, v.mask, + v.block_size, ) ) curr_block += num_file_block @@ -244,11 +252,9 @@ def merge_dataset(dst: str, src: str): v.nbytes, v.nlines, v.mask, + v.block_size, ) ) curr_block += num_file_block - random_fname = os.path.join(dst, \".meta.bin.%s\" % _random_string()) - with open(random_fname, \"wb\") as f: - pickle.dump(nw_info, f) - os.rename(random_fname, meta_path_dst) + _write_info_list(meta_path_dst, nw_info) diff --git a/cpm-live/cpm_live/generation/__init__.py b/cpm-live/cpm_live/generation/__init__.py new file mode 100644 index 0000000..70de125 --- /dev/null +++ b/cpm-live/cpm_live/generation/__init__.py @@ -0,0 +1 @@ +from .ant import CPMAntBeamSearch, CPMAntRandomSampling, CPMAntGeneration diff --git a/cpm-live/c"
}